# Main entrypoint of the workflow. 
# Please follow the best practices: 
# https://snakemake.readthedocs.io/en/stable/snakefiles/best_practices.html,
# in particular regarding the standardized folder structure mentioned there. 


include: "workflow/common.smk"


SAMPLES = list(METADATA.index)

get_bc_pattern = _get_default('bc_pattern')
get_cell_number = _get_default('cell_number')

paths = create_path_accessor()

rule all:
    input: expand(paths.kraken.output, sample=METADATA, db=['virus'])

def find_path(wc):
    "Return the R1_path or R2_path the given sample."
    return METADATA.loc[str(wc.sample), "R{}_path".format(wc.num)]

# symlink read one and two to resources/
rule symlink_reads:
    output:
        "resources/{sample}_R{num}.fastq.gz",
    params:
        path=find_path
    shell:
        "ln -fs `readlink -f {params.path}` {output}"

rule umi_tools_whitelist:
    input: 
        read_one="resources/{sample}_R{num}.fastq.gz",
    output: 
        paths.umi_tools_whitelist.whitelist
    params:
        bc_pattern=get_bc_pattern,
        cell_number=get_cell_number,
    conda:
        "envs/umi_tools.yaml"
    log:
        "results/logs/whitelist/{sample}_whitelist.log"
    shell:
        "umi_tools whitelist "
        "--stdin {input.read_one} "
        "--bc-pattern={params.bc_pattern} "
        "--set-cell-number={params.cell_number} "
        "--log2stderr > {output} 2> {log} "


rule umi_tools_extract:
    input: 
        read_one="resources/{sample}_R1.fastq.gz",
        read_two="resources/{sample}_R2.fastq.gz",
        whitelist=paths.umi_tools_whitelist.whitelist,
    output: 
        read_one=paths.umi_tools_extract.read_one,
        read_two=paths.umi_tools_extract.read_two,
    params:
        bc_pattern=get_bc_pattern,
        cell_number=get_cell_number,
    conda:
        "envs/umi_tools.yaml"
    log:
        "results/logs/whitelist/{sample}_whitelist.log"
    shell:
        "umi_tools extract "
        "--stdin {input.read_one} "
        "--bc-pattern={params.bc_pattern} "
        "--stdout {output.read_one} "
        "--read2-in {input.read_two} "
        "--read2-out {output.read_two} "
        "--whitelist={input.whitelist} "

def krak_db(wc):
    return config['kraken']['db_paths'][str(wc.db)]

rule kraken:
    input: 
        read_one=paths.umi_tools_extract.read_one,
        read_two=paths.umi_tools_extract.read_two,
    output: 
        kraken_report=paths.kraken.report,
        kraken_output=paths.kraken.output,
    threads: 
        config['kraken']['threads']
    params:
        db=krak_db
    conda: 
        "envs/kraken2.yaml"
    shell:
        "kraken2 --db {params.db} "
        "--threads {threads} "
        "--report {output.kraken_report} "
        "{input} "
        "> {output.kraken_output} "

# TODO: how to deal with clustering/deduping reads if we can't use BAM files? 
# Do we even care about this? 
# c.f. https://github.com/CGATOxford/UMI-tools/issues/436
# also https://pubmed.ncbi.nlm.nih.gov/30351359/
# Personally I think we want to use the UMIclusterer class, and if the 
# reads are in the same UMI cluster *and* are mapping to the same kraken
# db, then we deduplicate them. 

rule get_synapse:
    output: 
        "downloads/{sample}_R{num}.fastq.gz"
    conda:
        "envs/synapse.yaml"
    params:

    shell:
        "synapse -"